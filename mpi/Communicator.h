//
// Created by User on 30.06.2019.
//

#ifndef MPI2_COMMUNICATOR_H
#define MPI2_COMMUNICATOR_H

#include <exception>
#include <iostream>

#include "exceptions.h"
#include "mpi.h"
#include "Group.h"
#include "Datatype.h"
#include "Request.h"
#include "Operation.h"


namespace mpi {

    class Group;
    class CartesianCommunicator;

    /**
     * Represents the communicator.
     *
     * Communicator is a basic conception of MPI. Communicator defines a certain group of the processes and allows
     * to transmit information between processes within this. This information may be transmitted only between those
     * processes that belong to the communicator.
     *
     * The communicator contains methods that send or receive the information. The information sent by a certain
     * communicator can't be received by another communicator
     *
     * Isolated communicators are communicators that contain just one process. Any information transmission between
     * isolated communicators is impossible, the most of data transmission routines will finish with errors When
     * casting the communicator to bool object, the isolated communicators will cast to false while all other
     * communicators will cast to true
     *
     * Example:
     * if (comm){
     *          ... // data exchange by means of comm
     * }
     *
     */
    class Communicator{
    private:
        int numprocs = -1;
        int rank = -1;

        void* buffer = nullptr;
        bool isBufferAttached() const { return buffer != nullptr; }
        int bSize = -1;
        int bUnitSize = -1;
    protected:
        MPI_Comm comm;
        bool deletable;
    public:

        /**
         * A basic method to create the communicator. You don't need to use this constructor if
         * you don't apply classic MPI C method (mpi.h library)
         *
         * @param an object generated by classic MPI routines
         */
        Communicator(MPI_Comm c): comm(c) {
            deletable = (comm != MPI_COMM_WORLD && comm != MPI_COMM_NULL);
        };

        Communicator(const Communicator& other) = delete;

        /**
         * Only rvalues can be used for copying constructor.
         * The communicator can't be created from another communicator. Use mpi::Communicator, mpi::Group or
         * mpi::App routines to create the communicator object.
         *
         * @param other
         *
         * See App::getAppCommunicator() for details
         */
        Communicator(Communicator&& other){
            comm = other.comm;
            numprocs = other.numprocs;
            rank = other.rank;
            buffer = other.buffer;
            bSize = other.bSize;
            bUnitSize = other.bUnitSize;
            deletable = other.deletable;
            other.comm = 0;
            other.numprocs = -1;
            other.rank = -01;
            other.deletable = false;
        }

        /**
         * Moves the communicator other to the current to the current communicator. The communicator 'other'
         * will be destroyed. Please bear in mind that operator = moves the communicator, don't copy it.
         *
         * After comm1 = comm2
         * 1. comm1 will receive all messages sent by comm2 before such assignment
         *
         * Example:
         * comm2.send(...);
         * comm1 = comm2l
         * comm1.recv(...); // will receive the message sent by comm2
         *
         * 2. comm2 becomes 'bad'
         *
         * Example:
         * comm2.send(...);
         * comm1 = comm2;
         * comm2.recv(...); // will throw an exception
         *
         * @param other
         * @return *this
         */
        virtual Communicator& operator=(Communicator&& other){
            comm = other.comm;
            numprocs = other.numprocs;
            rank = other.rank;
            buffer = other.buffer;
            bSize = other.bSize;
            bUnitSize = other.bUnitSize;
            deletable = other.deletable;
            other.comm = MPI_COMM_NULL;
            other.numprocs = -1;
            other.rank = -1;
            other.buffer = nullptr;
            other.bSize = 0;
            other.bUnitSize = 0;
            other.deletable = false;
            return *this;
        }

        /**
         * Destroys the communicator. The method is mandatory.
         */
        virtual ~Communicator(){
            if (isBufferAttached()){
                std::cerr << "[ERROR] The buffer has been attached. It can't be detached manually\n";
            }
            if (deletable){
                int errcode;
                if ((errcode = MPI_Comm_free(&comm)) != MPI_SUCCESS){
                    switch (errcode){
                        case MPI_ERR_COMM:
                            std::cerr << "[ERROR] Failure to free the communicator because because the communicator" <<
                                         " became invalid\n";
                            break;
                        case MPI_ERR_ARG:
                        default:
                            std::cerr << "[ERROR] Unable to destroy the communicator\n";
                    }
                }
            } else {

            }
        }

        /**
         * When the function requires MPI_Comm instance, not mpi::Communicator one, just
         * apply the * operation to the communicator
         *
         * @return the required data
         */
        const MPI_Comm& operator*() const{
            return comm;
        }

        /**
         * @return true is the communicator is isolated, false otherwise
         */
        bool operator!() const{
            return comm == MPI_COMM_NULL;
        }

        /**
         * @return true if the communicator is not isolated.
         */
        operator bool() const{
            return comm != MPI_COMM_NULL;
        }

        /**
         * Returns the total number of all processes belonging to the communicator
         *
         * @return
         */
        int getProcessorNumber(){
            if (numprocs == -1){
                if (comm != MPI_COMM_NULL) {
                    int errcode = MPI_Comm_size(comm, &numprocs);
                    if (errcode != MPI_SUCCESS){
                        throw_exception(errcode);
                    }
                } else {
                    numprocs = 1;
                }
            }

            return numprocs;
        }

        /**
         * Returns the rank of the calling process
         *
         * @return the rank
         */
        int getRank(){
            if (rank == -1){
                if (comm != MPI_COMM_NULL) {
                    int errcode = MPI_Comm_rank(comm, &rank);
                    if (errcode != MPI_SUCCESS){
                        throw_exception(errcode);
                    }
                } else {
                    rank = 0;
                }
            }

            return rank;
        }

        /**
         * Provides the send of the data. Depending ont eh situation, the send can be blocking (i.e., the method
         * suspends execution of the program) or non-blocking (i.e., no suspend). The method don't guarantee
         * that buf is no longer used after the send
         *
         * @param buf - the data to be sent
         * @param count - number of items to send
         * @param datatype - type of sending items
         * @param dest - rank of the process that shall receive the data
         * @param tag - tag of the sending message. In order to transmit the information send and recv routines
         * shall be called with the same tag
         */
        void send(const void* buf, int count, MPI_Datatype datatype, int dest, int tag) const {
            if (comm == MPI_COMM_NULL){
                throw rank_error();
            }
            int errcode;
            if ((errcode = MPI_Send(buf, count, datatype, dest, tag, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * The same as send(...) but provides its non-blocking variant. This means that this function will not
         * complete the data send. In order to complete the data send use mpi::Request and mpi::Requests instances
         *
         * @param buf - see send(...) for details
         * @param count - see send(...) for details
         * @param datatype - see send(...) for details
         * @param dest - see send(...) for details
         * @param tag - see send(...) for details
         * @return the data that you may use in order to create mpi::Request or mpi::Requests instances
         */
        MPI_Request isend(const void* buf, int count, MPI_Datatype datatype, int dest, int tag) const{
            if (comm == MPI_COMM_NULL) throw rank_error();
            int errcode;
            MPI_Request request;
            if ((errcode = MPI_Isend(buf, count, datatype, dest, tag, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /** Sending the message using explicitly defined buffer
         *
         * @param buf - the buffer created and attached
         * @param count number of items
         * @param datatype - the datatype
         * @param dest - rank of the destination process
         * @param tag - the message tag
         */
        void bsend(const void* buf, int count, MPI_Datatype datatype, int dest, int tag) const{
            if (comm == MPI_COMM_NULL) throw rank_error();
            if (!isBufferAttached()) throw buffer_error();
            int errcode;
            if ((errcode = MPI_Bsend(buf, count, datatype, dest, tag, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /** Blocking ready send of the message
         *
         * @param buf - initial address of the buffer
         * @param count - number of elements in the buffer
         * @param datatype - the datatype
         * @param dest - rank of the destination process
         * @param tag - message tag
         */
        void rsend(const void* buf, int count, MPI_Datatype datatype, int dest, int tag) const{
            int errcode;
            if ((errcode = MPI_Rsend(buf, count, datatype, dest, tag, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * Creates a persistent request for a standard send. The send will be initialized but not completed.
         * However, you may force the completion of this request manually
         *
         * @param buf - see send(...) for details
         * @param count  - see send(...) for details
         * @param dtype - see send(...) for details
         * @param dest - see send(...) for details
         * @param tag - see send(...) for details
         * @return the data that shall be used for creating instances of mpi::Request or mpi::Requests. Once
         * the instances were created, their start() methods shall be run in order to complete the requests
         */
        Request sendInit(const void* buf, int count, MPI_Datatype dtype, int dest, int tag) const {
            if (comm == MPI_COMM_NULL) throw rank_error();
            int errcode;
            MPI_Request request;
            if ((errcode = MPI_Send_init(buf, count, dtype, dest, tag, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            Request obj(request);
            return obj;
        }

        /**
         * Receives the message. The function is blocking. This means that execution of the process will be
         * suspended until the awaiting data were actually sent
         *
         * @param buf - the buffer where the receiving data shall be placed
         * @param count - number of receiving items
         * @param datatype - type of receiving items
         * @param source - the process which data shall be accepted (optional, by default the data will be accepted
         * from all processes
         * @param tag - tag of the message (optional). If this argument was set, the function will accept only those
         * messages which were sent with the tag coincided with the argumemt
         * @return the information containing the message tag and rank of the source process
         */
        MPI_Status recv(void* buf, int count, MPI_Datatype datatype, int source = MPI_ANY_SOURCE,
                int tag = MPI_ANY_TAG) const{
            if (comm == MPI_COMM_NULL){
                throw rank_error();
            }
            int errcode;
            MPI_Status status;
            if ((errcode = MPI_Recv(buf, count, datatype, source, tag, comm, &status)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return status;
        }

        /**
         * The same as recv(...) but does it non-blockingly. This means that this function doesn't suspend
         * the process called it but doesn't guarantee that all data will ne received.
         *
         * @param buf - see recv(...) for details
         * @param count - see recv(...) for details
         * @param datatype - see recv(...) for details
         * @param source - see recv(...) for details
         * @param tag - see recv(...) for details
         * @return the data that will be substituted to mpi::Request or mpi::Requests constructors. Use instances
         * of these classes to force this request to complete
         */
        MPI_Request irecv(void* buf, int count, MPI_Datatype datatype,
                int source = MPI_ANY_SOURCE, int tag = MPI_ANY_TAG) const{
            if (comm == MPI_COMM_NULL) throw rank_error();
            int errcode;
            MPI_Request request;
            if ((errcode = MPI_Irecv(buf, count, datatype, source, tag, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * The same as recvInit but starts the receive.
         *
         * @param buf - see recv(...) for details
         * @param count - see recv(...) for details
         * @param datatype - see recv(...) for details
         * @param source - see recv(...) for details
         * @param tag - see recv(...) for details
         * @return the data that shall be used in mpi::Request or mpi::Requests constructors. Use instances of these
         * objects to force the data receive
         */
        MPI_Request recvInit(void* buf, int count, MPI_Datatype datatype, int source, int tag) const{
            MPI_Request request_content;
            int errcode;
            if ((errcode = MPI_Recv_init(buf, count, datatype, source, tag, comm, &request_content)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request_content;
        }

        /**
         * Provides symmetrical send and receive the data. The function will not be completed until all data were send
         *
         * @param sendbuf - buffer containing the data to send
         * @param sendcount - number of items to send
         * @param sendtype - type of the sending data
         * @param dest - rank the will accept the sending data
         * @param sendtag - tag of the outcome message
         * @param recvbuf - buffer where receiving data will be placed
         * @param recvcount - number of receiving items
         * @param recvtype - type of receiving items
         * @param source - optional, if put, rank of the process which data will be received
         * @param recvtag - optional, if put, tag of the message that shall be received
         * @return information containing rank of the process which message has been received and tag of the received
         * message
         */
        MPI_Status sendrecv(const void* sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag,
                      void* recvbuf, int recvcount, MPI_Datatype recvtype,
                      int source = MPI_ANY_SOURCE, int recvtag = MPI_ANY_TAG) const{
            if (comm == MPI_COMM_NULL) throw rank_error();
            int errcode;
            MPI_Status status;
            if ((errcode = MPI_Sendrecv(sendbuf, sendcount, sendtype, dest, sendtag, recvbuf, recvcount, recvtype,
                    source, recvtag, comm, &status)) != MPI_SUCCESS){
                throw_exception(errcode);
            };
            return status;
        }

        /**
         * Tests whether the message was sent by another process, but don't receive the message
         *
         * @param source - rank of the process which message shall be checked
         * @param tag - tag of the message to check
         * @return the information about the message
         */
        MPI_Status probe(int source = MPI_ANY_SOURCE, int tag = MPI_ANY_TAG) const{
            int errcode;
            MPI_Status status;
            if ((errcode = MPI_Probe(source, tag, comm, &status)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return status;
        }



        /** Returns true if the message from 'source' has been delivered but doesn't receive the delivered
         * message
         *
         * @param source - the source rank. Default - check messages from all processes
         * @param tag - the source tag. Default - check messages with all tags
         * @param status - the message status
         * @return true if the message has been received and false otherwise
         */
        bool iprobe(int source = MPI_ANY_SOURCE, int tag = MPI_ANY_TAG, MPI_Status* status = MPI_STATUS_IGNORE) const{
            int errcode;
            int flag;
            if ((errcode = MPI_Iprobe(source, tag, comm, &flag, status)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return flag;
        }


        /**
         * Sends the information from a certain processes to all other processes within the communicator
         *
         * buffer           buffer
         * A (root)     ->  A
         * X            ->  A
         * X            ->  A
         * X            ->  A
         *
         *
         * @param buffer - a pointer to the information that shall be sent (mustr be available for count items)
         * @param count - number of items to send
         * @param datatype - datatype of each item
         * @param root - rank of the process that sends the data (so called root process)
         */
        void broadcast(void* buffer, int count, MPI_Datatype datatype, int root) const{
            int errcode;
            if (comm == MPI_COMM_NULL) return;
            if ((errcode = MPI_Bcast(buffer, count, datatype, root, comm))!=MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * The same as broadcast but doesn't block the program execution and doesn't complete the request
         *
         * @param buffer see broadcast(...) for details
         * @param count see broadcast(...) for details
         * @param dtype see broadcast(...) for details
         * @param root see broadcast(...) for details
         * @return an MPI_Request. This value shall be used in order to complete the request. For details see
         * mpi::Request and mpi::Requests help
         */
        MPI_Request ibroadcast(void* buffer, int count, MPI_Datatype dtype, int root) const{
            if (comm == MPI_COMM_NULL){
                return MPI_REQUEST_NULL;
            }
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Ibcast(buffer, count, dtype, root, comm, &request) != MPI_SUCCESS)){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * Sends information from a certain process individually to each process in the communicator
         *
         * sendbuf      ->  recvbuf
         * A B C D          A   (if this is the root process)
         * X X X X          B
         * X X X X          C
         * X X X X          D
         *
         *
         * @param sendbuf - the buffer that shall contain all sending data in the root process. The memory of the buffer
         * shall be allocated for numprocs*sendcount items where numprocs is total number of all processes in the
         * communicator. The information starting from sendbuf + rank * sendcount and finishing at
         * sendbuf + (rank+1) * sendcount - 1 will be sent to the process # rank
         * @param sendcount - total number of items to send to each process
         * @param sendtype - datatype to send
         * @param recvbuf - the buffer where all receiving data shall be put. The memory shall be allocated for
         * recvcount items
         * @param recvcount - number of items to receive
         * @param recvtype - datatype of the receiving items
         * @param root - the root process (the process that will send all the data)
         */
        void scatter(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                void* recvbuf, int recvcount, MPI_Datatype recvtype, int root) const{
            int errcode;
            if ((errcode = MPI_Scatter(sendbuf, sendcount, sendtype,
                    recvbuf, recvcount,recvtype, root, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * Sends all information from the root process (a process which rank was passed as root argument) to all
         * processes within the communication using the following transmission scheme
         *
         * sendbuf+displs[0]    sendbuf+disps[1]    sendbuf+disps[2]    sendbuf+displs[3]    recvbuf
         * X                    X                   X                   X                    A
         * X                    X                   X                   X                    B
         * A                    B                   C                   D                    C (the root process)
         * X                    X                   X                   X                    D
         *
         * @param sendbuf - the buffer contains the data to send
         * @param sendcounts[j] - number of data to send to j-th process
         * @param displs[j] - the first item that will send to j-th process (its address relatively to sendbuf)
         * @param sendtype - type of each data
         * @param recvbuf - the buffer where the data received will be acquired
         * @param recvcount - number of items received by each process (max.)
         * @param recvtype - type of the receiving data
         * @param root - the root process, or the process that will send the data
         */
        void scatter(const void* sendbuf, const int* sendcounts, const int* displs,
                MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype,
                int root) const{
            int errcode;
            if ((errcode = MPI_Scatterv(sendbuf, sendcounts, displs, sendtype,
                    recvbuf, recvcount, recvtype, root, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * The same as scatter(...) but does it non-blocking. This means that it doesn't suspends the program
         * execution but doesn't finish the process
         *
         * @param sendbuf - see scatter(...) for details
         * @param sendcount - see scatter(...) for details
         * @param sendtype - see scatter(...) for details
         * @param recvbuf - see scatter(...) for details
         * @param recvcount - see scatter(...) for details
         * @param recvtype - see scatter(...) for details
         * @param root - see scatter(...) for details
         * @return the number of the request. The number shall be substituted to mpi::Request or mpi::Requests
         * instances. See help for these classes for details
         */
        MPI_Request iscatter(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                void* recvbuf, int recvcount, MPI_Datatype recvtype, int root) const{
            int errcode;
            MPI_Request request;
            if ((errcode = MPI_Iscatter(sendbuf, sendcount, sendtype,
                    recvbuf, recvcount, recvtype, root, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }


        /**
         * The same as scatter(...) but does it non-blocking. This means that it doesn't suspends the program
         * execution but doesn't finish the process
         *
         * @param sendbuf - see scatter(...) for details
         * @param sendcounts - see scatter(...) for details
         * @param displs - see scatter(...) for details
         * @param sendtype - see scatter(...) for details
         * @param recvbuf - see scatter(...) for details
         * @param recvcount - see scatter(...) for details
         * @param recvtype - see scatter(...) for details
         * @param root - see scatter(...) for details
         * @return the number of the request. The number shall be substituted to mpi::Request or mpi::Requests
         * instances. See help for these classes for details
         */
        MPI_Request iscatter(const void* sendbuf, const int* sendcounts, const int* displs,
                MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype,
                int root) const{
            int errcode;
            MPI_Request request;
            if ((errcode = MPI_Iscatterv(sendbuf, sendcounts, displs, sendtype,
                    recvbuf, recvcount, recvtype, root, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * Sends the data to the process with the rank named root (or the root process
         *
         * sendbuf  ->  recvbuf
         * A        ->  X X X X
         * B        ->  A B C D (if this is the root process)
         * C        ->  X X X X
         * D        ->  X X X X
         *
         *
         * @param sendbuf - the buffer containing the data that will be send to the root process. The buffer shall be
         * allocated for sendbuf items
         * @param sendcount - number of items to send
         * @param sendtype - data type of each item
         * @param recvbuf - the buffer containing the data that will be received by the root process. For the root
         * process the buffer shall be allocated by recvcount * numprocs items where numprocs is total number of
         * all processes in the communicator. The process with i-th rank will send the data containing in memory
         * staring from i * recvcount and finishing at (i+1) * recvcount - 1
         * @param recvcount - number of items to receive
         * @param recvtype - datatype of each receiving item
         * @param root - the process that will receive all the data (the root process)
         */
        void gather(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                void* recvbuf, int recvcount, MPI_Datatype recvtype, int root) const{
            if (comm == MPI_COMM_NULL) throw communicator_error();
            int errcode;
            if ((errcode = MPI_Gather(sendbuf, sendcount, sendtype,
                    recvbuf, recvcount, recvtype, root, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * Sends the data from all processes belonging to the communicator to the root process (i.e.,
         * the process which rank was passed to root argument of this function
         *
         * The data transmission will be organized according to the following scheme:
         * sendbuf      recvbuf + displs[0] recvbuf + displs[1] recvbuf + displs[2] recvbuf + displs[3]
         * A            X                   X                   X                   X
         * B            X                   X                   X                   X
         * C            A                   B                   C                   D               (the root process)
         * D            X                   X                   X                   X
         *
         * @param sendbuf contains the data that will be sent to the root process
         * @param sendcount - number of items sent by each process to the root process
         * @param sendtype - type of sending data
         * @param recvbuf - the buffer where all receiving data will be placed
         * @param recvcounts[j] defines the number of items received from j-th process
         * @param displs[j] - defines the location of start of the data received by j-th process, in recvbuf array
         * @param recvtype - type of the receiving items
         * @param root - the root process, or the process that will receive all the data
         */
        void gather(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                void* recvbuf, const int* recvcounts, const int* displs, MPI_Datatype recvtype,
                int root) const{
            int errcode;
            if ((errcode = MPI_Gatherv(sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype,
                    root, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * The same as gather(...) but provides non-blocking data transmission. This means that the execution
         * of this function will not suspend your program but will not complete the data transmission
         *
         * @param sendbuf - see gather(...) for details
         * @param sendcount - see gather(...) for details
         * @param sendtype - see gather(...) for details
         * @param recvbuf - see gather(...) for details
         * @param recvcount - see gather(...) for details
         * @param recvtype - see gather(...) for details
         * @param root - see gather(...) for details
         * @return the number of the request. The number shall be substituted to mpi::Request or mpi::Requests
         * instances. See help for these classes for details
         */
        MPI_Request igather(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                     void* recvbuf, int recvcount, MPI_Datatype recvtype, int root) const{
            int errcode;
            MPI_Request request;
            if ((errcode = MPI_Igather(sendbuf, sendcount, sendtype,
                    recvbuf, recvcount, recvtype, root, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }


        /**
         * The same as gather(...) but provides non-blocking data transmission. This means that the execution
         * of this function will not suspend your program but will not complete the data transmission
         *
         * @param sendbuf - see gather(...) for details
         * @param sendcount - see gather(...) for details
         * @param sendtype - see gather(...) for details
         * @param recvbuf - see gather(...) for details
         * @param recvcounts - see gather(...) for details
         * @param displs - see gather(...) for details
         * @param recvtype - see gather(...) for details
         * @param root - see gather(...) for details
         * @return the number of the request. The number shall be substituted to mpi::Request or mpi::Requests
         * instances. See help for these classes for details
         */
        MPI_Request igather(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
               void* recvbuf, const int* recvcounts, const int* displs, MPI_Datatype recvtype,
               int root) const{
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Igatherv(sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype,
                    root, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * The ssame as gather but all processes will be treated as the root process and hence will receive all data
         * sent by all processes
         *
         * sendbuf  ->  recvbuf
         * A        ->  A B C D
         * B        ->  A B C D
         * C        ->  A B C D
         * D        ->  A B C D
         *
         *
         * @param sendbuf - the data to be sent. The buffer shall be allocated for sencdount items
         * @param sendcount - number of items to send
         * @param sendtype - type of each sending item
         * @param recvbuf - the data to be received
         * @param recvcount - number of items to be received. The buffer shall be allocated for recvcount * numprocs
         * items where numprocs is total number of all processes in the communicator
         * @param recvtype - type of the receiving items
         */
        void allGather(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                void* recvbuf, int recvcount, MPI_Datatype recvtype) const{
            if (comm == MPI_COMM_NULL) throw communicator_error();
            int errcode;
            if ((errcode = MPI_Allgather(sendbuf, sendcount, sendtype,
                    recvbuf, recvcount, recvtype, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }


        /**
         * The ssame as gather but all processes will be treated as the root process and hence will receive all data
         * sent by all processes
         *
         * sendbuf  ->  recvbuf + displs[0]     recvbuf + displs[1]     recvbuf + displs[2] recvbuf + displs[3]
         * A        ->  A                       B                       C                   D
         * B        ->  A                       B                       C                   D
         * C        ->  A                       B                       C                   D
         * D        ->  A                       B                       C                   D
         *
         * @param sendbuf - the buffer containing all items to send. The memory shall be allocated by
         * sendcount items
         * @param sendcount - total number of items to send by each process
         * @param sendtype - type of sending items
         * @param recvbuf - start of the buffer that contains all receiving items. The buffer shall be allocated
         * for appropriate number of items
         * @param recvcount - total number of counts received from each process
         * @param displs[i] - start of the array region that contains the data received by i-th process
         * @param rectype - type of the receiving items
         */
        void allGather(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                void* recvbuf, const int* recvcount, const int* displs, MPI_Datatype rectype) const{
            if (comm == MPI_COMM_NULL) throw communicator_error();
            int errcode;
            if ((errcode = MPI_Allgatherv(sendbuf, sendcount, sendtype,
                    recvbuf, recvcount, displs, rectype, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * The same as allGather but does it non-blockingly. This means that this routine will not suspend your
         * application but will not finish the data transmission. In order to finish the data transmission
         * use the result returned by this routine and see mpi::Request and mpi::Requests for details
         *
         * @param sendbuf - see allGather(...) for details
         * @param sendcount - see allGather(...) for details
         * @param sendtype - see allGather(...) for details
         * @param recvbuf - see allGather(...) for details
         * @param recvcount - see allGather(...) for details
         * @param recvtype - see allGather(...) for details
         * @return the data that shall be substituted int mpi::Request or mpi::Requests
         */
        MPI_Request iallGather(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                               void* recvbuf, int recvcount, MPI_Datatype recvtype) const {
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Iallgather(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype,
                    comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * The same as allGather but does it non-blockingly. This means that this routine will not suspend your
         * application but will not finish the data transmission. In order to finish the data transmission
         * use the result returned by this routine and see mpi::Request and mpi::Requests for details
         *
         * @param sendbuf - see allGather(...) for details
         * @param sendcount - see allGather(...) for details
         * @param sendtype - see allGather(...) for details
         * @param recvbuf - see allGather(...) for details
         * @param recvcount - see allGather(...) for details
         * @param displs - see allGather(...) for details
         * @param rectype - see allGather(...) for details
         * @return the data that shall be substituted int mpi::Request or mpi::Requests
         */
        MPI_Request iallGather(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                  void* recvbuf, const int* recvcount, const int* displs, MPI_Datatype rectype) const{
            int errcode;
            MPI_Request request;
            if ((errcode = MPI_Iallgatherv(sendbuf, sendcount, sendtype, recvbuf, recvcount, displs, rectype,
                    comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * Distributes the data along all processes using the following scheme
         *
         * sendbuf      ->  recvbuf
         * A0 A1 A2 A3      A0 B0 C0 D0
         * B0 B1 B2 B3      A1 B1 C1 D1
         * C0 C1 C2 C3      A2 B2 C2 D2
         * D0 D1 D2 D3      A3 B3 C3 D3
         *
         * @param sendbuf - buffer that contains all sending items. The buffer shall be allocated for
         * sendcount * numprocs items. The data starting from i * sencount will be sent to the process with
         * rank number i
         * @param sendcount - number of items to be send by each process
         * @param sendtype - type of sending items
         * @param recvbuf - buffer that contains all received data. The buffer shall be allocated for
         * recvcount * numprocs items. The data received fro process with rank number i will be placed to
         * the memory started from recvbuf + i * recvcount
         * @param recvcount - total number of items to be received
         * @param recvtype - type of the receiving items
         */
        void allToAll(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                void* recvbuf, int recvcount, MPI_Datatype recvtype) const{
            int errcode;
            if ((errcode = MPI_Alltoall(sendbuf, sendcount, sendtype,
                    recvbuf, recvcount, recvtype, comm))!= MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * Exchanges the data between all processes in the communicator using the following scheme
         *
         * The data to send:
         * sendbuf+sdispls[0]   sendbuf+sdispls[1]  sendbuf+sdispls[2]  sendbuf+sdispls[3]
         * A0                   A1                  A2                  A3
         * B0                   B1                  B2                  B3
         * C0                   C1                  C2                  C3
         * D0                   D1                  D2                  D3
         *
         * The data to receive:
         * recvbuf+rdispls[0]   recvbuf+rdispls[1]  recvbuf+rdispls[2]  recvbuf+rdispls[3]
         * A0                   B0                  C0                  D0
         * A1                   B1                  C1                  D1
         * A2                   B2                  C2                  D2
         * A3                   B3                  C3                  D3
         *
         * @param sendbuf - the buffer that contains all data to send
         * @param sendcounts[j] - number of items that will be sent by j-th process
         * @param sdispls[j] - starting index of the data that will be sent by j-th process
         * @param sendtype - type of the sending data
         * @param recvbuf - the buffer that will contain all recevied data
         * @param recvcounts[j] - number of items that will be received by jth process
         * @param rdispls[j] - starting index of the data that will be received by j-th process
         * @param recvtype - type of the receiving data
         */
        void allToAll(const void* sendbuf, const int* sendcounts, const int* sdispls, MPI_Datatype sendtype,
                void* recvbuf, const int* recvcounts, const int* rdispls, MPI_Datatype recvtype) const{
            int errcode;
            if ((errcode = MPI_Alltoallv(sendbuf, sendcounts, sdispls, sendtype,
                    recvbuf, recvcounts, rdispls, recvtype, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * Exchanges the data between all processes in the communicator using the following scheme
         *
         * The data to send:
         * sendbuf+sdispls[0]   sendbuf+sdispls[1]  sendbuf+sdispls[2]  sendbuf+sdispls[3]
         * A0                   A1                  A2                  A3
         * B0                   B1                  B2                  B3
         * C0                   C1                  C2                  C3
         * D0                   D1                  D2                  D3
         *
         * The data to receive:
         * recvbuf+rdispls[0]   recvbuf+rdispls[1]  recvbuf+rdispls[2]  recvbuf+rdispls[3]
         * A0                   B0                  C0                  D0
         * A1                   B1                  C1                  D1
         * A2                   B2                  C2                  D2
         * A3                   B3                  C3                  D3
         *
         * @param sendbuf - the buffer that contains all data to send
         * @param sendcounts[j] - number of items that will be sent by j-th process
         * @param sdispls[j] - starting index of the data that will be sent by j-th process
         * @param sendtypes[j] - type of the data sent bu j-th process
         * @param recvbuf - the buffer that will contain all recevied data
         * @param recvcounts[j] - number of items that will be received by jth process
         * @param rdispls[j] - starting index of the data that will be received by j-th process
         * @param recvtypes[j] - the data that will be received by j-th process
         */
        void allToAll(const void* sendbuf, const int sendcounts[], const int sdispls[], const MPI_Datatype sendtypes[],
                void* recvbuf, const int recvcounts[], const int rdispls[], const MPI_Datatype recvtypes[]) const{
            int errcode;
            if ((errcode = MPI_Alltoallw(sendbuf, sendcounts, sdispls, sendtypes,
                    recvbuf, recvcounts, rdispls, recvtypes,
                    comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * the same as allToAll but does it in non-blocking manner. This means that the data transmission may
         * occur in parallel to the following code execution. This function just starts to transmit the data
         * The data transmission will be finished only if special routines will be called (such as
         * Request::wait, Requests::waitAll, Requests::waitAny
         *
         * @param sendbuf - see allToAll(...) for details
         * @param sendcount - see allToAll(...) for details
         * @param sendtype - see allToAll(...) for details
         * @param recvbuf - see allToAll(...) for details
         * @param recvcount - see allToAll(...) for details
         * @param recvtype - see allToAll(...) for details
         * @return the data that shall be substituted int mpi::Request or mpi::Requests
         */
        MPI_Request iallToAll(const void* sendbuf, int sendcount, MPI_Datatype sendtype,
                      void* recvbuf, int recvcount, MPI_Datatype recvtype) const{
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Ialltoall(sendbuf, sendcount, sendtype,
                    recvbuf, recvcount, recvtype, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * the same as allToAll but does it in non-blocking manner. This means that the data transmission may
         * occur in parallel to the following code execution. This function just starts to transmit the data
         * The data transmission will be finished only if special routines will be called (such as
         * Request::wait, Requests::waitAll, Requests::waitAny
         *
         * @param sendbuf - see allToAll(...) for details
         * @param sendcounts - see allToAll(...) for details
         * @param sdispls - see allToAll(...) for details
         * @param sendtype - see allToAll(...) for details
         * @param recvbuf - see allToAll(...) for details
         * @param recvcounts - see allToAll(...) for details
         * @param rdispls - see allToAll(...) for details
         * @param recvtype - see allToAll(...) for details
         * @return the data that shall be substituted int mpi::Request or mpi::Requests
         */
        MPI_Request iallToAll(const void* sendbuf, const int* sendcounts, const int* sdispls, MPI_Datatype sendtype,
                              void* recvbuf, const int* recvcounts, const int* rdispls, MPI_Datatype recvtype) const{
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Ialltoallv(sendbuf, sendcounts, sdispls, sendtype,
                    recvbuf, recvcounts, rdispls, recvtype, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * the same as allToAll but does it in non-blocking manner. This means that the data transmission may
         * occur in parallel to the following code execution. This function just starts to transmit the data
         * The data transmission will be finished only if special routines will be called (such as
         * Request::wait, Requests::waitAll, Requests::waitAny
         *
         * @param sendbuf  - see allToAll(...) for details
         * @param sendcounts - see allToAll(...) for details
         * @param sdispls - see allToAll(...) for details
         * @param sendtypes - see allToAll(...) for details
         * @param recvbuf - see allToAll(...) for details
         * @param recvcounts - see allToAll(...) for details
         * @param rdispls - see allToAll(...) for details
         * @param recvtypes - see allToAll(...) for details
         * @return the data that shall be substituted int mpi::Request or mpi::Requests
         */
        MPI_Request iallToAll(
                const void* sendbuf, const int sendcounts[], const int sdispls[], const MPI_Datatype sendtypes[],
                void* recvbuf, const int recvcounts[], const int rdispls[], const MPI_Datatype recvtypes[]) const{
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Ialltoallw(sendbuf, sendcounts, sdispls, sendtypes,
                    recvbuf, recvcounts,rdispls, recvtypes,
                    comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }


        /**
         * Applies the collective operation to each data containing in the buffer and returns the result to the
         * root process
         *
         * sendbuf  -> recvbuf
         * A           X
         * B           X
         * C           A op B op C op D (the root process)
         * D           X
         *
         * @param sendbuf - the data to be sent, or buffer for the operation input
         * @param recvbuf - the data to be received, or buffer for the operation output
         * @param count - number of data to transmit
         * @param datatype - type of the transmitting items
         * @param op - instance of mpi::Operation of MPI_Op (see help for mpi::Operation for details)
         * @param root - the root process or the process that will accept all the data
         */
        void reduce(const void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op,
                    int root) const{
            int errcode;
            if (comm == MPI_COMM_NULL) throw communicator_error();
            if ((errcode = MPI_Reduce(sendbuf, recvbuf, count, datatype, op, root, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        };

        /**
         * The same as reduce(...) but does it non-blockingly. This means that the job will not be finished
         * after the finish the function execution and the information about the job will be returned by the function
         *
         * @param sendbuf - see reduce(...) for details
         * @param recvbuf - see reduce(...) for details
         * @param count - see reduce(...) for details
         * @param datatype - see reduce(...) for details
         * @param op - see reduce(...) for details
         * @param root - see reduce(...) for details
         * @return
         */
        MPI_Request ireduce(const void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op,
               int root) const{
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Ireduce(sendbuf, recvbuf, count, datatype, op, root, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         *  The same as reduce() but uses all processes as root processes:
         *
         *  sendbuf         recvbuf
         *  A               A op B op C
         *  B               A op B op C
         *  C               A op B op C
         *
         * @param sendbuf - the data to be sent, or buffer for the operation input
         * @param recvbuf - the data to be received, or buffer for the operation output
         * @param count - number of data to transmit
         * @param datatype - type of the transmitting items
         * @param op - instance of mpi::Operation of MPI_Op (see help for mpi::Operation for details)
         */
        void allReduce(const void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op) const{
            if (comm == MPI_COMM_NULL) throw communicator_error();
            int errcode;
            if ((errcode = MPI_Allreduce(sendbuf, recvbuf, count, datatype, op, comm)) != MPI_SUCCESS){\
                throw_exception(errcode);
            }
        }

        /**
         * The same as allReduce, but does it non-blockingly. The function will not suspend the program execution
         * but will not accomplish the data transmission. The data shall be accomplished by creating mpu::Request
         * and mpi::Requests instances. See help on these classes for details
         *
         * @param sendbuf - see allReduce(...) for details
         * @param recvbuf - see allReduce(...) for details
         * @param count - see allReduce(...) for details
         * @param datatype - see allReduce(...) for details
         * @param op - see allReduce(...) for details
         * @return the data to substitute as arguments to mpi::Request or mpi::Requests constructor
         */
        MPI_Request iallReduce(const void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op) const{
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Iallreduce(sendbuf, recvbuf, count, datatype, op, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * The same as allReduce but the data receiving by the process with rank r will be computed based only on
         * data with processes with ranks 0, .. r
         *
         * sendbuf  recvbuf
         * A        A
         * B        A op B
         * C        A op B op C
         * D        A op B op C op D
         *
        * @param sendbuf - the data to be sent, or buffer for the operation input
         * @param recvbuf - the data to be received, or buffer for the operation output
         * @param count - number of data to transmit
         * @param datatype - type of the transmitting items
         * @param op - instance of mpi::Operation of MPI_Op (see help for mpi::Operation for details)
         */
        void scan(const void* sendbuf, void* recvbuf, int count, MPI_Datatype dtype, MPI_Op op) const{
            int errcode;
            if ((errcode = MPI_Scan(sendbuf, recvbuf, count, dtype, op, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * The same as scan(...) but does it non-blockingly. This doesn't suspend your application and doesn't
         * complete the data transmission
         *
         * @param sendbuf - see scan(...) for details
         * @param recvbuf - see scan(...) for details
         * @param count - see scan(...) for details
         * @param dtype - see scan(...) for details
         * @param op - see scan(...) for details
         * @return the data to substitute to mpi::Request or mpi::Requests instances. See help on these classes
         * for details
         */
        MPI_Request iscan(const void* sendbuf, void* recvbuf, int count, MPI_Datatype dtype, MPI_Op op) const{
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Iscan(sendbuf, recvbuf, count, dtype, op, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * The same as allReduce but the data receiving by the process with rank r will be computed based only on
         * data with processes with ranks 0, .. r-1
         *
         * sendbuf  recvbuf
         * A        0
         * B        A
         * C        A op B
         * D        A op B op C
         *
        * @param sendbuf - the data to be sent, or buffer for the operation input
         * @param recvbuf - the data to be received, or buffer for the operation output
         * @param count - number of data to transmit
         * @param datatype - type of the transmitting items
         * @param op - instance of mpi::Operation of MPI_Op (see help for mpi::Operation for details)
         */
        void exScan(const void* sendbuf, void* recvbuf, int count, MPI_Datatype dtype, MPI_Op op) const{
            int errcode;
            if ((errcode = MPI_Exscan(sendbuf, recvbuf, count, dtype, op, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * The ssame as exScan but provides the non-blocking collective.
         *
         * @param sendbuf - see exScan(...) for details
         * @param recvbuf - see exScan(...) for details
         * @param count - see exScan(...) for details
         * @param dtype - see exScan(...) for details
         * @param op - see exScan(...) for details
         * @return the data to substitute to mpi::Request and mpi::Requests constructor
         */
        MPI_Request iexScan(const void* sendbuf, void* recvbuf, int count, MPI_Datatype dtype, MPI_Op op) const{
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Iexscan(sendbuf, recvbuf, count, dtype, op, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }

        /**
         * being called from the process with rank r, it applies the operation op to all the data containing
         * in sendbuf, at location started from sendbuf + recvcounts[0] + ... + recvcounts[r-1] with length recvcountr[r]
         * and places the result to recvbuf
         *
         * sendbuf      recvbuf
         * A0 A1 A2 A3  A0 op B0 op C0 op D0
         * B0 B1 B2 B3  A1 op B1 op C1 op D1
         * C0 C1 C3 C3  A2 op B2 op C2 op D2
         * D0 D1 D2 D3  A3 op B3 op C3 op D3
         * start of A1 is defined by recvcounts[0], start of A2 is defined by recvcounts[0] + recvcounts[1] etc.
         *
         * @param sendbuf - the buffer containing all the data
         * @param recvbuf - the buffer where the data shall be placed
         * @param recvcounts - number of items received by each process
         * @param dtype - type of the items
         * @param op - an instance of MPI_Op or mpi::Operation (see mpi::Operation for details)
         */
        void reduceScatter(const void* sendbuf, void* recvbuf, const int recvcounts[],
                MPI_Datatype dtype, MPI_Op op) const{
            int errcode;
            if ((errcode = MPI_Reduce_scatter(sendbuf, recvbuf, recvcounts, dtype, op, comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * The same as reduceScatter but provides the non-blocking routine. The routine will not block
         * the execution of your own code but will not complete the job. Use the data returned by the function
         * to complete the job
         *
         * @param sendbuf - see reduceScatter(...) for details
         * @param recvbuf - see reduceScatter(...) for details
         * @param recvcounts - see reduceScatter(...) for details
         * @param dtype - see reduceScatter(...) for details
         * @param op - see reduceScatter(...) for details
         * @return the data to be substituted into constructor of mpi::Request or mpi::Requests. You shall create
         * instance of any of this class in order to retrieve information about the job or force the job to complete
         */
        MPI_Request ireduceScatter(const void* sendbuf, void* recvbuf,
                const int recvcounts[], MPI_Datatype dtype, MPI_Op op) const{
            MPI_Request request;
            int errcode;
            if ((errcode = MPI_Ireduce_scatter(sendbuf, recvbuf, recvcounts, dtype, op, comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }




        /**
         * Suspends the execution of all processes belonging to the communicator until all of them start this function
         */
        void barrier() const{
            if (comm == MPI_COMM_NULL) return;
            int errcode;
            if ((errcode = MPI_Barrier(comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
        }

        /**
         * Notifies if the process has reached the line where you put this method
         *
         * @return a structure to be substituted to mpi::Request and mpi::Requests. The structure contains
         * all necessary information. See help on these classes for details
         */
        MPI_Request ibarrier() const{
            int errcode;
            MPI_Request request;
            if ((errcode = MPI_Ibarrier(comm, &request)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return request;
        }






        /* Message buffer */

        /** Attaches the buffer that shall be used by bsend routine
         *
         *
         * @tparam T - type of the data to send
         * @param size - the buffer size
         */
        template<typename T> void attachBuffer(int size){
            if (isBufferAttached()){
                std::cerr << "[ERROR] The buffer has already attached\n";
                return;
            }
            int _bUnitSize = sizeof(T);
            int _bSize = (size + 2 * MPI_BSEND_OVERHEAD);
            buffer = (void*)(new T[_bSize]);
            int errcode;
            if ((errcode = MPI_Buffer_attach(buffer, _bSize * _bUnitSize)) != MPI_SUCCESS){
                delete [] buffer;
                throw_exception(errcode);
            }
            bSize = _bSize;
            bUnitSize = _bUnitSize;
        }

        /**
         * Detaches the buffer attached by the attachBuffer function
         *
         * @tparam T - type of the data used
         */
        template<typename T> void detachBuffer(){
            if (!isBufferAttached()) return;
            int desiredSize = sizeof(T);
            if (desiredSize != bUnitSize){
                std::cerr << "[ERROR] Template arguments in attaching and detaching buffer are not similar\n";
                throw buffer_error();
            }
            int errcode;
            int totalSize = bSize * bUnitSize;
            if ((errcode = MPI_Buffer_detach(buffer, &totalSize)) != MPI_SUCCESS){
                std::cerr << "Buffer detach was occured with error\n";
            }
            delete [] (T*)buffer;
            buffer = nullptr;
        }







        /* Construction of new communicators */

        /**
         * Creates the communicator that contains all processes in the current communicator but not those which
         * ranks are mentioned
         *
         * @tparam V - any iterable container (shall contain begin and end methods)
         * @param user_list - list of processes to exclude
         * @return the new communicator without the excluded processes
         */
        template<typename V> Communicator exclude(V user_list) const;

        /**
         * Excludes all processes belonging to the certain ranke
         *
         * @param start - the lowest rank belonging to the range
         * @param stop - the highest rank belonging to the range
         * @param step - step
         * @return the new communicator without the excluded processes
         */
        Communicator exclude(int start, int stop, int step = 1) const;

        /**
         * Creates new communicator that contains only those processes that belong to the certain communicator
         *
         * @tparam V the container type. The container shall have begin and end methods that returns necessary iterators
         * @param proc_list - list of all processes
         * @return new communicator that contains ll processes mentioned in the list
         */
        template<typename V> Communicator include(V proc_list) const;

        /**
         * Creates new communicator that contains only processes belonging to the certain range
         *
         * @param start - rank of the lowest process in the range
         * @param stop - rank of the higher process in the range
         * @param step - the step rank.
         * @return the created communicator
         */
        Communicator include(int start, int stop, int step = 1) const;



        /**
         * Creates new communicators based on colors and keys
         *
         * @param color some value greater than zero. All processes with the same color will belong to the same communicator
         * @param key the desired rank of the process in the new communicator
         * @return the created communicator
         */
        Communicator split(int color, int key) const{
            if (comm == MPI_COMM_NULL) throw communicator_error();
            int errcode;
            MPI_Comm newcomm;
            if ((errcode = MPI_Comm_split(comm, color, key, &newcomm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return Communicator(newcomm);
        }

        /**
         * Makes a new communicator that has cartesian topology
         *
         * @param ndims - number of dimensions
         * @param dims - grid size along each dimension
         * @param periods[j] - true if j-th dimension is periodic
         * @param reorder - true if the process ranks shall not be saved
         * @return an instance of mpi::CartesianCommunicator
         */
        CartesianCommunicator createCartesianTopology(int ndims, const int dims[], const bool periods[], bool reorder) const;

        /**
         * Creates an exact copy of this communicator
         *
         * @return the new communicator
         */
        Communicator duplicate() const{
            MPI_Comm new_comm;
            int errcode;
            if ((errcode = MPI_Comm_dup(comm, &new_comm)) != MPI_SUCCESS){
                throw_exception(errcode);
            }
            return Communicator(new_comm);
        }


    };

    /**
     * Returns the number of elements in the status
     *
     * @param status the MPI_Staus
     * @param dtype - the datatype
     * @return the number of elements
     */
    int get_count(MPI_Status& status, MPI_Datatype dtype);


#ifndef GROUP_OBJECT_CODE

    template<typename V> Communicator Communicator::exclude(V user_list) const {
        if (comm == MPI_COMM_NULL) throw communicator_error();
        mpi::Group g0(this);
        mpi::Group g1 = g0.exclude(user_list);
        return g1.createCommunicator();
    }

    template<typename V> Communicator Communicator::include(V proc_list) const{
        if (comm == MPI_COMM_NULL) throw communicator_error();
        Group g0(this);
        Group g1 = g0.include(proc_list);
        return g1.createCommunicator();
    }


#endif


}



#endif //MPI2_COMMUNICATOR_H
